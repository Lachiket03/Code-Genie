Our main goal is to predict future stock prices — either the next 5 days or up to 200 days, depending on the setup.
This helps investors make smarter decisions by identifying short-term opportunities and long-term trends.
It supports things like timing entries and exits, or adjusting investment strategies.


 Model Architecture
"To do this, we built a deep learning model using a 2-layer LSTM network — which is well-suited for time-series data like stock prices."

LSTM Layers:

"These two LSTM layers allow the model to learn long-term patterns — not just day-to-day changes, but how those changes evolve over time."

Dropout Layers:

"We added dropout layers after each LSTM to reduce overfitting. That means the model generalizes better to unseen data instead of just memorizing training data."

Dense Output Layer:

"Finally, we use a Dense layer to produce the prediction.
In this case, it outputs multiple values at once — for example, the next 5 days of stock prices — instead of just one."



All the scripts make use of LSTM (Long Short-Term Memory) networks — a type of Recurrent Neural Network (RNN) that's particularly effective for time-series prediction, like stock price forecasting.

Why Adam is a good choice here:
Efficient on large datasets like stock market data.

Handles noisy or sparse gradients well.

Often used for time-series models like LSTMs due to its stability and speed.

MSE measures the average of the squares of the errors — that is, the average squared difference between predicted and actual values:

One of the biggest issues with regular RNNs is something called the vanishing gradient problem.

When the network tries to learn from long sequences — like a hundred days of stock prices — the learning signal, or gradient, becomes so small that the early layers basically stop learning. It’s like trying to shout across a football field — your voice just fades out.

LSTMs solve this by using a structure called a cell state, which acts like a conveyor belt for information. It can carry important data across many time steps without letting it fade away.

And the cool part? LSTM has gates — like input, forget, and output gates — that control what to keep, what to forget, and what to use. That’s how it avoids the vanishing gradient problem and remembers long-term patterns like trends or momentum in stock prices."


Sliding Window:

"We use a 60-day historical window — meaning the model sees the past 60 days of data to make a prediction. This is called a 'look_back window'.

Scaling:

"All features are scaled between 0 and 1 using a MinMaxScaler. This keeps the data balanced so that no feature dominates due to scale differences — like Volume being in millions vs RSI in decimals.


What the Model Learns
"The model isn't just memorizing prices — it's learning deeper patterns, such as:"

How technical indicators relate to price behavior — for example, how RSI or MACD affect upcoming price changes.

How momentum, trading volume, and even market sentiment from news or social media contribute to price direction.

"Basically, it learns the ‘cause-and-effect’ between these features and price changes over time."

Input: The past 60 days of features like close price, RSI, MACD, etc.

Output: The model then predicts the next 5 days of closing prices.

"This is known as a sliding window approach — we keep feeding it rolling 60-day chunks to generate continuous 5-day forecasts."

✅ Benefits of This Setup
"This deployment approach offers several advantages:"

Modular training: Since each model is trained per stock, we can retrain or update them independently without affecting others.

Fast & scalable: Predictions are quick, making it suitable for production or even live market use.

User-friendly: The Streamlit interface makes it easy for both technical and non-technical users to visualize predictions and gain insights without needing to understand the model code."
